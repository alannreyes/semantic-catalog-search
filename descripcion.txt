# Descripción del Sistema de Búsqueda Semántica de Productos

## 1. Estructura de Datos

### 1.1 Tabla de Productos (productos_1024)
- Campos principales:
  * codigo_efc (VARCHAR): Identificador único del producto
  * descripcion (TEXT): Descripción detallada del producto
  * marca (VARCHAR): Marca del producto
  * codfabrica (VARCHAR): Código de fábrica
  * articulo_stock (INTEGER): Flag de disponibilidad en stock (0/1)
  * lista_costos (INTEGER): Flag de acuerdo de precios con proveedor (0/1)
  * embedding (vector(1024)): Vector de embedding generado con text-embedding-3-large
  * uuid (UUID): Identificador interno de PostgreSQL (no usado en lógica de negocio)

Ejemplo:
```sql
{
  "codigo_efc": "EFC123456",
  "descripcion": "Llave ajustable 10 pulgadas marca Stanley",
  "marca": "STANLEY",
  "codfabrica": "ST-10A",
  "articulo_stock": 1,
  "lista_costos": 0,
  "embedding": "[0.1, 0.2, ..., 0.3]", -- 1024 dimensiones
  "uuid": "550e8400-e29b-41d4-a716-446655440000"
}
```

### 1.2 Tabla de Marcas (marcas)
- Campos:
  * marca (VARCHAR): Nombre de la marca (identificador)
  * segment (VARCHAR): Segmento de la marca (premium, standard, economy)

Ejemplo:
```sql
{
  "marca": "STANLEY",
  "segment": "premium"
}
```

## 2. Sistema de Búsqueda

### 2.1 Proceso de Búsqueda
1. Recepción de Query
   - Ejemplo: "llave ajustable stanley 10 pulgadas"
   - Parámetros opcionales:
     * limit: número de resultados (default: 5)
     * segment: preferencia de segmento (premium, standard, economy)

2. Generación de Embedding
   - Usa modelo text-embedding-3-large
   - Convierte texto en vector de 1024 dimensiones
   - Ejemplo: "llave ajustable" → [0.1, 0.2, ..., 0.3]

3. Búsqueda Vectorial
   - Compara similitud coseno entre vectores
   - Obtiene segmento de cada producto desde tabla marcas
   - Si marca no tiene segmento, asume "standard"

4. Aplicación de Boost por Segmento
   - Cuando segment = premium:
     * Productos premium: 1.3x (30% más)
     * Productos standard: 1.2x (20% más)
     * Productos economy: 1.0x (sin boost)
   
   - Cuando segment = economy:
     * Productos economy: 1.3x (30% más)
     * Productos standard: 1.2x (20% más)
     * Productos premium: 1.0x (sin boost)
   
   - Cuando segment = standard o no se especifica:
     * Todos los productos: 1.0x (sin boost)
    
   Ejemplo (segment = premium):
   ```
   Producto A (Premium): 0.8 * 1.3 = 1.04 → 1.0 (máximo)
   Producto B (Standard): 0.9 * 1.2 = 1.08 → 1.0 (máximo)
   Producto C (Economy): 0.95 * 1.0 = 0.95 → 0.95
   ```

   Ejemplo (segment = economy):
   ```
   Producto A (Premium): 0.8 * 1.0 = 0.8 → 0.8
   Producto B (Standard): 0.9 * 1.2 = 1.08 → 1.0 (máximo)
   Producto C (Economy): 0.95 * 1.3 = 1.235 → 1.0 (máximo)
   ```

   **Nota importante**: El valor de similitud final nunca puede exceder 1.0, por lo que cualquier resultado que supere este límite se redondea a 1.0.

5. Selección con GPT
   - Analiza resultados considerando:
     * Similitud vectorial
     * Boost por segmento
     * Preferencia de segmento del usuario
   - Clasifica resultado como:
     * EXACTO: Coincidencia perfecta
     * EQUIVALENTE: Misma función, especificaciones similares
     * COMPATIBLE: Sirve para el mismo propósito
     * ALTERNATIVO: Puede servir con diferencias
     * DISTINTO: No es lo buscado

### 2.2 Endpoints de Búsqueda

1. POST /search
```json
{
  "query": "llave ajustable stanley 10 pulgadas",
  "limit": 5,
  "segment": "premium"
}
```

2. GET /webhook/:id
```
/webhook/123?query=llave ajustable stanley 10 pulgadas&limit=5&segment=premium
```

## 3. Sincronización con MS SQL

### 3.1 Campos Sincronizados
- codigo_efc (ART_CODART)
- descripcion (ART_DESART)
- marca (ART_PARAM3)
- codfabrica (ART_CODFABRICA)
- articulo_stock (ART_FLGSTKDIST)
- lista_costos (ART_FLGLSTPRE)

### 3.2 Lógica de Sincronización
1. **Actualización Masiva (Estrategia A)**
   - Sobrescribe todos los productos existentes
   - Para cada producto:
     * Limpia y homologa descripción (FEGA→Fierro Galvanizado, FENO→Fierro Negro)
     * Genera nuevo embedding con texto limpio
     * Actualiza todos los campos en PostgreSQL
   - Ventaja: Asegura consistencia total
   - Desventaja: Costo en embeddings de OpenAI

2. **Limpieza y Homologación de Descripciones**
   - **IMPORTANTE**: MS SQL mantiene acrónimos originales (para guías/facturas)
   - **Traducción solo para embeddings**: El sistema traduce antes de OpenAI
   - Tabla de mapeo de acrónimos EFC:
     * FEGA → Fierro Galvanizado
     * FENO → Fierro Negro
     * [Lista completa pendiente del administrador del maestro]
   - Proceso antes de generar embedding:
     1. **Detectar y reemplazar acrónimos** por términos completos
     2. Normalizar espacios y caracteres especiales
     3. Validar longitud y contenido
     4. **Generar embedding con texto traducido**
   - **Beneficio**: Embeddings entienden materiales reales, no códigos internos

3. **Adición de Productos Nuevos**
   - Genera embedding con descripción limpia y homologada
   - Inserta en PostgreSQL con todos los campos

4. **Eliminación de Productos**
   - Solo cuando se indica explícitamente
   - No hay lógica automática de comparación

### 3.3 Query de Origen (MS SQL)
```sql
-- Query proporcionado por DBA (referencia para campos y filtros)
SELECT 
  ART_CODART as codigo_efc,
  ART_DESART as descripcion,
  ART_PARAM3 as marca,
  ART_CODFABRICA as codfabrica,
  ISNULL(ART_FLGSTKDIST, 0) as articulo_stock,
  ISNULL(ART_FLGLSTPRE, 0) as lista_costos
FROM Ar0000 
WHERE ART_CODFAM <= '47' 
  AND ART_ESTREG = 'A'
ORDER BY ART_CODART
```

### 3.4 Ejemplo de Procesamiento
```sql
-- Descripción original en MS SQL (SE MANTIENE IGUAL)
"Tubo FEGA 1/2 pulgada marca Stanley"

-- Texto traducido SOLO para embedding (interno del sistema)
"Tubo Fierro Galvanizado 1/2 pulgada marca Stanley"

-- PostgreSQL: Descripción original + embedding del texto traducido
UPDATE productos_1024
SET descripcion = 'Tubo FEGA 1/2 pulgada marca Stanley',  -- Original para facturas
    marca = 'STANLEY',
    codfabrica = 'ST-TUBE-12',
    articulo_stock = 1,
    lista_costos = 0,
    embedding = '[vector_generado_con_texto_traducido]'  -- Embedding del texto limpio
WHERE codigo_efc = 'EFC123456'
```

**Flujo de Procesamiento:**
1. MS SQL: `"Tubo FEGA 1/2 pulgada"` (original)
2. Sistema: Traduce a `"Tubo Fierro Galvanizado 1/2 pulgada"` (temporal)
3. OpenAI: Genera embedding del texto traducido
4. PostgreSQL: Guarda descripción original + embedding traducido

## 4. Consideraciones Importantes

1. Identificadores
   - codigo_efc: Identificador principal para operaciones CRUD
   - uuid: Solo para PostgreSQL, no usado en lógica

2. Segmentos
   - No se almacenan en tabla de productos
   - Se obtienen en tiempo de ejecución
   - Default: "standard" si no existe

3. Rendimiento
   - Optimizado para ~1 millón de productos
   - No hay comparación automática de productos
   - Búsqueda vectorial con índice IVFFlat

4. Seguridad
   - Validación de parámetros de entrada
   - Manejo de errores y excepciones
   - Logging de operaciones críticas 

## 5. ETAPA 2: Sistema de Migración Masiva Parametrizable

### 5.1 Objetivo
Crear un sistema profesional para migración de datos entre cualquier base de datos origen y PostgreSQL con embeddings, tolerante a fallas y monitoreable en tiempo real.

### 5.2 Arquitectura del Sistema

#### 5.2.1 Componentes Principales
1. **Migration Controller**: Maneja endpoints REST
2. **Migration Service**: Lógica de procesamiento
3. **Database Connectors**: Adaptadores para diferentes DB
4. **Job Manager**: Control de estado y progreso
5. **Embedding Processor**: Generación de vectores por lotes
6. **Progress Monitor**: Tracking y notificaciones

#### 5.2.2 Tabla de Control de Jobs
```sql
CREATE TABLE migration_jobs (
  id UUID PRIMARY KEY,
  status VARCHAR(20), -- pending, running, paused, completed, failed
  source_config JSONB,
  destination_config JSONB,
  processing_config JSONB,
  progress JSONB, -- { total: 1000000, processed: 250000, errors: 12 }
  created_at TIMESTAMP,
  started_at TIMESTAMP,
  completed_at TIMESTAMP,
  error_log TEXT[]
);
```

### 5.3 Flujo de Procesamiento Detallado

#### Fase 1: Inicialización (30 segundos)
1. Validar conexiones origen y destino
2. Verificar estructura de tablas
3. Estimar total de registros
4. Crear job en tabla de control
5. Limpiar tabla destino (si clean_before=true)

#### Fase 2: Procesamiento por Lotes (8-12 horas)
```
Para cada lote de 500 registros:
├── Extraer datos de DB origen (5-10 seg)
├── Validar datos requeridos (1 seg)
├── Procesar embeddings en sublotes de 50 (30-60 seg)
│   ├── Llamada a OpenAI API
│   ├── Manejo de rate limits
│   └── Retry en caso de error
├── Insertar en PostgreSQL (2-5 seg)
└── Actualizar progreso (1 seg)
```

#### Fase 3: Finalización (10 segundos)
1. Crear índices vectoriales
2. Actualizar estadísticas
3. Marcar job como completado
4. Generar reporte final

### 5.4 Endpoints de la API

#### 5.4.1 Iniciar Migración
```
POST /migration/bulk-load
Content-Type: application/json

{
  "source": {
    "type": "mssql",
    "connection": {
      "host": "192.168.1.100",
      "port": 1433,
      "database": "productos_db",
      "user": "migration_user",
      "password": "secure_pass"
    },
    "table": "productos_master",
         "fields": {
       "codigo_efc": "ART_CODART",
       "descripcion": "ART_DESART",
       "marca": "ART_PARAM3",
       "codfabrica": "ART_CODFABRICA",
       "articulo_stock": "ART_FLGSTKDIST",
       "lista_costos": "ART_FLGLSTPRE"
     },
         "where_clause": "ART_CODFAM <= '47' AND ART_ESTREG = 'A'"
  },
  "destination": {
    "table": "productos_1024_v2",
    "clean_before": true,
    "create_indexes": true
  },
     "processing": {
     "batch_size": 500,
     "embedding_batch_size": 50,
     "max_concurrent_embeddings": 3,
     "delay_between_batches_ms": 1000,
     "retry_attempts": 3,
     "text_cleaning": {
       "enabled": true,
       "acronym_mapping": {
         "FEGA": "Fierro Galvanizado",
         "FENO": "Fierro Negro"
       }
     }
   },
  "notifications": {
    "progress_interval": 1000, // Cada 1000 registros
    "webhook_url": "https://mi-sistema.com/webhook/progress" // Opcional
  }
}

Response:
{
  "job_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "pending",
  "estimated_total": 1250000,
  "estimated_duration_hours": 10.5
}
```

#### 5.4.2 Consultar Estado
```
GET /migration/jobs/{job_id}/status

Response:
{
  "job_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "running",
  "progress": {
    "total_records": 1250000,
    "processed": 340000,
    "errors": 15,
    "percentage": 27.2,
    "current_batch": 680,
    "records_per_second": 95.5,
    "estimated_remaining_minutes": 198
  },
  "timings": {
    "started_at": "2024-01-15T10:30:00Z",
    "estimated_completion": "2024-01-15T23:45:00Z"
  },
  "last_error": "Rate limit exceeded, retrying in 5 seconds"
}
```

#### 5.4.3 Control de Jobs
```
POST /migration/jobs/{job_id}/pause   - Pausar migración
POST /migration/jobs/{job_id}/resume  - Reanudar migración
POST /migration/jobs/{job_id}/cancel  - Cancelar migración
GET  /migration/jobs                  - Listar todos los jobs
```

### 5.5 Manejo de Errores y Tolerancia a Fallas

#### 5.5.1 Tipos de Errores
1. **Conexión DB**: Retry automático 3 veces
2. **Rate Limit OpenAI**: Espera progresiva (5s, 10s, 20s)
3. **Datos inválidos**: Skip record, log error
4. **Memoria insuficiente**: Reducir batch_size automáticamente
5. **Timeout**: Resume desde último lote exitoso

#### 5.5.2 Estrategia de Recovery
- **Checkpoint cada 100 lotes**: Guarda progreso en DB
- **Resume automático**: Al reiniciar, continúa desde último checkpoint
- **Rollback parcial**: Si falla un lote, solo reintenta ese lote

### 5.6 Optimizaciones de Performance

#### 5.6.1 Conexiones de Base de Datos
- Pool separado para migración (10 conexiones)
- Timeout configurables
- Heartbeat para mantener conexiones activas

#### 5.6.2 Procesamiento de Embeddings
- Queue de trabajo con workers paralelos
- Batch dinámico según rate limits
- Cache de embeddings para descripciones duplicadas

#### 5.6.3 Inserción en PostgreSQL
- Transacciones por lote (500 registros)
- COPY en lugar de INSERT para mayor velocidad
- Índices creados al final para mejor performance

### 5.7 Monitoreo y Alertas

#### 5.7.1 Métricas en Tiempo Real
- Registros procesados por segundo
- Rate de errores
- Uso de memoria y CPU
- Latencia de OpenAI API

#### 5.7.2 Notificaciones
- Webhook de progreso cada X registros
- Email en caso de errores críticos
- Slack/Teams integration (opcional)

### 5.8 Preparación para Etapa 3: Sistema de Ranking Avanzado

#### 5.8.1 Campos Preparados
- `articulo_stock`: Para priorizar productos disponibles
- `lista_costos`: Para priorizar productos con acuerdos de precio

#### 5.8.2 Fórmula de Ranking Futura
```
Ranking Final = Similitud_Coseno + Boost_Segmento + Boost_Stock + Boost_Lista_Precios

Donde:
- Boost_Stock = +0.1 si articulo_stock = 1
- Boost_Lista_Precios = +0.05 si lista_costos = 1
```

#### 5.8.3 Visualización Futura
- 🟢 Productos en stock (articulo_stock = 1)
- 🔵 Productos con acuerdo de precios (lista_costos = 1)
- 🟡 Productos con ambos beneficios
- ⚪ Productos normales

### 5.9 Casos de Uso Futuros
1. **Sistema de acrónimos dinámico** (administración de mapeos)
2. **Sincronización incremental** usando timestamps
3. **Multi-tenant** para diferentes clientes
4. **Dashboard web** para administración visual
5. **Integración con BIP** para workflows empresariales

## ✅ **PASO 3 COMPLETADO: Sistema de Procesamiento por Lotes (Enero 2024)**

### 📋 Funcionalidades Implementadas

#### 🔄 Flujo de Procesamiento Completo
1. **Lectura por lotes desde MS SQL** (DatabaseService.getDataBatch)
   - Paginación con OFFSET/FETCH 
   - Manejo de filtros WHERE parametrizables
   - Pool de conexiones optimizado (10 conexiones, 5min timeout)

2. **Traducción de acrónimos** (MigrationService.processTextCleaning)
   - Integración con sistema de acrónimos dinámico
   - Preserva texto original para base de datos
   - Genera texto traducido solo para embeddings
   - Optimización: Skip si text_cleaning.enabled = false

3. **Generación de embeddings** (MigrationService.generateEmbeddings)
   - Procesamiento en sublotes (default: 50 registros)
   - Rate limiting automático (1 segundo entre sublotes)
   - Manejo de errores: continúa con null embedding si falla
   - Soporte text-embedding-3-large con 1024 dimensiones

4. **Inserción en PostgreSQL** (MigrationService.insertBatchToPostgreSQL)
   - Upsert automático con ON CONFLICT(codigo_efc)
   - Conversión de tipos (articulo_stock/lista_costos a 0/1)
   - Manejo individual de errores por registro
   - Vector formato string: [0.1,0.2,...,0.n]

#### ⚡ Optimizaciones de Performance
- **Lotes de 500 registros** para procesamiento general
- **Sublotes de 50 embeddings** para respetar rate limits OpenAI
- **Delay configurable** entre lotes (default: 1 segundo)
- **Índices automáticos** al final del proceso (IVFFlat + codigo_efc)

#### 🔧 Control de Errores y Tolerancia a Fallas
- **Retry por lotes**: Máximo 3 intentos antes de abortar
- **Error logging**: Almacena cada error en migration_jobs.error_log
- **Continuidad**: Un lote fallido no detiene la migración completa
- **Background processing**: No bloquea respuesta HTTP del endpoint

#### 📊 Monitoreo en Tiempo Real
- **Progreso detallado**: processed/total, percentage, current_batch
- **Métricas de performance**: records_per_second, estimated_remaining_minutes
- **Status tracking**: pending → running → completed/failed
- **Timestamping**: created_at, started_at, completed_at

### 🚀 Endpoints REST Implementados

```bash
# 1. Crear job de migración (con validaciones y defaults)
POST /migration/bulk-load

# 2. Iniciar procesamiento en background
POST /migration/jobs/{jobId}/start

# 3. Consultar progreso en tiempo real
GET /migration/jobs/{jobId}/status

# 4. Listar todos los jobs
GET /migration/jobs

# 5. Test de conectividad MS SQL
POST /migration/test-connection
```

### 🧪 Script de Pruebas
**Archivo**: `test-migration.js`
- Test completo del flujo end-to-end
- Configuración realista con datos EFC (tabla Ar0000)
- Monitoreo automático de progreso cada 3 segundos
- Manejo de errores y timeouts

**Uso**:
```bash
npm install axios  # Solo si no está instalado
node test-migration.js
```

### 📈 Métricas de Performance Esperadas
- **Velocidad**: ~100 registros/minuto (incluyendo embeddings)
- **Memoria**: Eficiente con procesamiento por lotes
- **Rate limits**: Respeta límites de OpenAI automáticamente
- **Escalabilidad**: Hasta 1M+ registros sin problemas

### 🔄 Flujo de Datos Detallado
```
MS SQL (Ar0000) 
  ↓ [Lote de 500 con filtros WHERE]
DatabaseService.getDataBatch()
  ↓ [Datos crudos campo por campo]
MigrationService.processTextCleaning()
  ↓ [Original para DB + traducido para embedding]
MigrationService.generateEmbeddings()
  ↓ [Sublotes de 50 + rate limiting + error recovery]
OpenAI text-embedding-3-large API
  ↓ [Vectores 1024D normalizados]
MigrationService.insertBatchToPostgreSQL()
  ↓ [Upsert por codigo_efc + conversión tipos]
PostgreSQL (productos_1024) con índices automáticos
```

### 📁 Archivos Creados/Modificados en Paso 3
- `src/migration/migration.service.ts` - Lógica completa de procesamiento
- `src/migration/migration.controller.ts` - Endpoint /start actualizado
- `src/migration/database.service.ts` - getDataBatch() implementado
- `test-migration.js` - Script de pruebas end-to-end
- `descripcion.txt` - Documentación actualizada

## ✅ **PASO 4 COMPLETADO: Controles Avanzados de Migración (Enero 2024)**

### 🎛️ Funcionalidades de Control Implementadas

#### 🎮 Endpoints de Control Avanzado
1. **POST /migration/jobs/{jobId}/pause** - Pausar migración en ejecución
2. **POST /migration/jobs/{jobId}/resume** - Reanudar migración pausada  
3. **POST /migration/jobs/{jobId}/cancel** - Cancelar migración (running/paused/pending)
4. **DELETE /migration/jobs/{jobId}** - Eliminar job (solo completed/failed/cancelled)

#### 🔄 Lógica de Estados
```
pending → running → completed
    ↓         ↓         ↑
    ↓    → paused → ────┘
    ↓         ↓
    └─→ cancelled
```

#### 🛡️ Validaciones de Estado
- **Pause**: Solo jobs en estado 'running'
- **Resume**: Solo jobs en estado 'paused' 
- **Cancel**: Jobs en estados 'running', 'paused', 'pending'
- **Delete**: Solo jobs en estados finales ('completed', 'failed', 'cancelled')

#### ⚡ Procesamiento en Tiempo Real
- **Verificación por lote**: El proceso verifica flags de control antes de cada lote
- **Pausa inmediata**: Al pausar, el job se detiene después del lote actual
- **Resume con contexto**: Al reanudar, continúa desde donde se pausó
- **Cancelación limpia**: Al cancelar, marca el estado y termina procesamiento

### 🧪 Script de Pruebas Avanzadas
**Archivo**: `test-advanced-controls.js`
- ✅ Prueba completa de pause/resume/cancel
- ✅ Validaciones de estados incorrectos  
- ✅ Test de eliminación de jobs
- ✅ Manejo de múltiples jobs simultáneos
- ✅ Cleanup automático en caso de errores

**Uso**:
```bash
node test-advanced-controls.js
```

### 🔧 Mejoras en el Sistema Base
- **Test de conexión real**: Endpoint /test-connection ahora prueba MS SQL
- **Flags de control**: Sistema de flags en processing_config para pause/cancel
- **Background processing**: Migración no bloquea API endpoints
- **Error handling mejorado**: Validaciones de estado más robustas

### 📊 Endpoints REST Completos

```bash
# 📋 Gestión de Jobs
POST   /migration/bulk-load           # Crear job
POST   /migration/jobs/{id}/start     # Iniciar procesamiento
GET    /migration/jobs/{id}/status    # Consultar estado
GET    /migration/jobs               # Listar todos

# 🎛️ Controles Avanzados  
POST   /migration/jobs/{id}/pause     # Pausar migración
POST   /migration/jobs/{id}/resume    # Reanudar migración
POST   /migration/jobs/{id}/cancel    # Cancelar migración
DELETE /migration/jobs/{id}           # Eliminar job

# 🔌 Utilidades
POST   /migration/test-connection     # Test MS SQL
```

### 📁 Archivos Finales Creados/Modificados
- `src/migration/migration.service.ts` - Métodos de control completos
- `src/migration/migration.controller.ts` - Endpoints REST avanzados
- `test-migration.js` - Prueba básica de migración
- `test-advanced-controls.js` - Prueba de controles avanzados
- `descripcion.txt` - Documentación completa actualizada

### 🎯 **SISTEMA COMPLETAMENTE FUNCIONAL**
- ✅ **Infraestructura PostgreSQL + MS SQL**
- ✅ **Sistema de acrónimos dinámico**
- ✅ **Procesamiento por lotes escalable**
- ✅ **Generación de embeddings OpenAI** 
- ✅ **Tolerancia a fallas y recovery**
- ✅ **Monitoreo en tiempo real**
- ✅ **Controles avanzados (pause/resume/cancel)**
- ✅ **API REST completa**
- ✅ **Scripts de prueba exhaustivos**

## 🚀 **ETAPA 2 DEL PROYECTO COMPLETADA**

El sistema de migración masiva está **listo para producción** y puede:

### ✨ Capacidades Principales
- **Migrar millones de registros** desde MS SQL a PostgreSQL
- **Generar embeddings** con OpenAI de forma eficiente y escalable
- **Traducir acrónimos** dinámicamente preservando texto original
- **Monitorear progreso** en tiempo real con métricas detalladas
- **Controlar ejecución** con pause/resume/cancel según necesidades
- **Manejar errores** con tolerancia a fallas y recovery automático
- **Ejecutar en background** sin bloquear operaciones del sistema

### 🎮 Flujo de Usuario Típico
1. **Crear job** con `POST /migration/bulk-load`
2. **Iniciar migración** con `POST /migration/jobs/{id}/start`
3. **Monitorear progreso** con `GET /migration/jobs/{id}/status`
4. **Controlar según necesidad**: pause/resume/cancel
5. **Completar migración** automáticamente o cancelar si es necesario

### 📈 Performance y Escalabilidad
- **~100 registros/minuto** (incluyendo embeddings)
- **Lotes configurables** (500 registros default)
- **Rate limiting inteligente** para OpenAI
- **Pool de conexiones optimizado** MS SQL y PostgreSQL
- **Memoria eficiente** con procesamiento por lotes

El sistema está preparado para manejar la **migración de datos completa de EFC** y puede extenderse fácilmente para futuras necesidades empresariales.